{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Blogs Dataset\n",
        "\n",
        "Running Modularity-Aware Graph Autoencoder on the Blogs dataset with specified hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Moher\\Personal\\PhD\\Projects\\modularity_aware_gae\\modularity_aware_gae\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "EXPERIMENTAL SETTING \n",
            "\n",
            "- Graph dataset: blogs\n",
            "- Mode name: gcn_vae\n",
            "- Number of models to train: 1\n",
            "- Number of training iterations for each model: 200\n",
            "- Learning rate: 0.01\n",
            "- Dropout rate: 0.0\n",
            "- Use of node features in the input layer: False\n",
            "- Dimension of the GCN hidden layer: 32\n",
            "- Dimension of the output layer: 16\n",
            "- lambda: 0.5\n",
            "- beta: 0.75\n",
            "- gamma: 2.0\n",
            "- s: 10\n",
            "- FastGAE: no \n",
            "\n",
            "Final embedding vectors will be evaluated on:\n",
            "- Task 2, i.e., joint community detection and link prediction\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "LOADING DATA\n",
            "\n",
            "Loading the blogs graph\n",
            "- Number of nodes: 1224\n",
            "- Number of communities: 2\n",
            "- Use of node features: False\n",
            "Done! \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "EXPERIMENTS ON MODEL 1 / 1 \n",
            "\n",
            "STEP 1/3 - PREPROCESSING STEPS \n",
            "\n",
            "Masking some edges from the training graph, for link prediction\n",
            "(validation set: 5.0 % of edges - test set: 10.0 % of edges)\n",
            "Done! \n",
            "\n",
            "Preprocessing node features\n",
            "Done! \n",
            "\n",
            "Running the Louvain algorithm for community detection\n",
            "as a preprocessing step for the encoder\n",
            "Done! Louvain has found 35 communities \n",
            "\n",
            "Setting up the model and the optimizer\n",
            "Done! \n",
            "\n",
            "Preprocessing on message passing matrices\n",
            "Done! \n",
            "\n",
            "Initializing TF session\n",
            "Done! \n",
            "\n",
            "STEP 2/3 - MODEL TRAINING \n",
            "\n",
            "Starting training\n",
            "Iteration: 0001 Loss: 0.09370 Time: 0.25915\n",
            "Iteration: 0002 Loss: 0.09292 Time: 0.07224\n",
            "Iteration: 0003 Loss: 0.08996 Time: 0.06358\n",
            "Iteration: 0004 Loss: 0.08351 Time: 0.05200\n",
            "Iteration: 0005 Loss: 0.07237 Time: 0.07156\n",
            "Iteration: 0006 Loss: 0.05595 Time: 0.07407\n",
            "Iteration: 0007 Loss: 0.03519 Time: 0.07051\n",
            "Iteration: 0008 Loss: 0.01432 Time: 0.07754\n",
            "Iteration: 0009 Loss: 0.00256 Time: 0.06052\n",
            "Iteration: 0010 Loss: 0.00913 Time: 0.06105\n",
            "Iteration: 0011 Loss: 0.01917 Time: 0.05347\n",
            "Iteration: 0012 Loss: 0.01655 Time: 0.06968\n",
            "Iteration: 0013 Loss: 0.00632 Time: 0.05161\n",
            "Iteration: 0014 Loss: -0.00342 Time: 0.06937\n",
            "Iteration: 0015 Loss: -0.00853 Time: 0.06801\n",
            "Iteration: 0016 Loss: -0.00925 Time: 0.05094\n",
            "Iteration: 0017 Loss: -0.00810 Time: 0.05069\n",
            "Iteration: 0018 Loss: -0.00739 Time: 0.05599\n",
            "Iteration: 0019 Loss: -0.00832 Time: 0.05001\n",
            "Iteration: 0020 Loss: -0.01112 Time: 0.04851\n",
            "Iteration: 0021 Loss: -0.01538 Time: 0.06006\n",
            "Iteration: 0022 Loss: -0.02027 Time: 0.06220\n",
            "Iteration: 0023 Loss: -0.02480 Time: 0.05848\n",
            "Iteration: 0024 Loss: -0.02814 Time: 0.05500\n",
            "Iteration: 0025 Loss: -0.03013 Time: 0.05552\n",
            "Iteration: 0026 Loss: -0.03149 Time: 0.05300\n",
            "Iteration: 0027 Loss: -0.03347 Time: 0.07151\n",
            "Iteration: 0028 Loss: -0.03674 Time: 0.06063\n",
            "Iteration: 0029 Loss: -0.04081 Time: 0.06000\n",
            "Iteration: 0030 Loss: -0.04420 Time: 0.06154\n",
            "Iteration: 0031 Loss: -0.04601 Time: 0.05700\n",
            "Iteration: 0032 Loss: -0.04791 Time: 0.06151\n",
            "Iteration: 0033 Loss: -0.05113 Time: 0.05753\n",
            "Iteration: 0034 Loss: -0.05511 Time: 0.07701\n",
            "Iteration: 0035 Loss: -0.05890 Time: 0.04851\n",
            "Iteration: 0036 Loss: -0.06231 Time: 0.05406\n",
            "Iteration: 0037 Loss: -0.06597 Time: 0.07747\n",
            "Iteration: 0038 Loss: -0.07056 Time: 0.06955\n",
            "Iteration: 0039 Loss: -0.07618 Time: 0.05100\n",
            "Iteration: 0040 Loss: -0.08240 Time: 0.05056\n",
            "Iteration: 0041 Loss: -0.08877 Time: 0.05301\n",
            "Iteration: 0042 Loss: -0.09544 Time: 0.04951\n",
            "Iteration: 0043 Loss: -0.10265 Time: 0.05194\n",
            "Iteration: 0044 Loss: -0.10967 Time: 0.07856\n",
            "Iteration: 0045 Loss: -0.11528 Time: 0.06456\n",
            "Iteration: 0046 Loss: -0.11852 Time: 0.05001\n",
            "Iteration: 0047 Loss: -0.11914 Time: 0.05927\n",
            "Iteration: 0048 Loss: -0.11769 Time: 0.07957\n",
            "Iteration: 0049 Loss: -0.11553 Time: 0.04800\n",
            "Iteration: 0050 Loss: -0.11407 Time: 0.05106\n",
            "Iteration: 0051 Loss: -0.11411 Time: 0.05400\n",
            "Iteration: 0052 Loss: -0.11560 Time: 0.06235\n",
            "Iteration: 0053 Loss: -0.11795 Time: 0.06995\n",
            "Iteration: 0054 Loss: -0.12048 Time: 0.05959\n",
            "Iteration: 0055 Loss: -0.12267 Time: 0.05158\n",
            "Iteration: 0056 Loss: -0.12427 Time: 0.04704\n",
            "Iteration: 0057 Loss: -0.12519 Time: 0.06455\n",
            "Iteration: 0058 Loss: -0.12543 Time: 0.06347\n",
            "Iteration: 0059 Loss: -0.12516 Time: 0.05200\n",
            "Iteration: 0060 Loss: -0.12468 Time: 0.05352\n",
            "Iteration: 0061 Loss: -0.12441 Time: 0.06600\n",
            "Iteration: 0062 Loss: -0.12456 Time: 0.06955\n",
            "Iteration: 0063 Loss: -0.12509 Time: 0.05200\n",
            "Iteration: 0064 Loss: -0.12582 Time: 0.06877\n",
            "Iteration: 0065 Loss: -0.12658 Time: 0.06053\n",
            "Iteration: 0066 Loss: -0.12729 Time: 0.05501\n",
            "Iteration: 0067 Loss: -0.12793 Time: 0.04715\n",
            "Iteration: 0068 Loss: -0.12842 Time: 0.04801\n",
            "Iteration: 0069 Loss: -0.12873 Time: 0.05253\n",
            "Iteration: 0070 Loss: -0.12882 Time: 0.05901\n",
            "Iteration: 0071 Loss: -0.12874 Time: 0.05251\n",
            "Iteration: 0072 Loss: -0.12862 Time: 0.05701\n",
            "Iteration: 0073 Loss: -0.12859 Time: 0.06756\n",
            "Iteration: 0074 Loss: -0.12872 Time: 0.06053\n",
            "Iteration: 0075 Loss: -0.12902 Time: 0.06201\n",
            "Iteration: 0076 Loss: -0.12939 Time: 0.07050\n",
            "Iteration: 0077 Loss: -0.12972 Time: 0.05152\n",
            "Iteration: 0078 Loss: -0.12998 Time: 0.05705\n",
            "Iteration: 0079 Loss: -0.13015 Time: 0.06648\n",
            "Iteration: 0080 Loss: -0.13028 Time: 0.06101\n",
            "Iteration: 0081 Loss: -0.13037 Time: 0.04964\n",
            "Iteration: 0082 Loss: -0.13043 Time: 0.05201\n",
            "Iteration: 0083 Loss: -0.13048 Time: 0.05459\n",
            "Iteration: 0084 Loss: -0.13055 Time: 0.05867\n",
            "Iteration: 0085 Loss: -0.13068 Time: 0.06800\n",
            "Iteration: 0086 Loss: -0.13085 Time: 0.05303\n",
            "Iteration: 0087 Loss: -0.13104 Time: 0.05996\n",
            "Iteration: 0088 Loss: -0.13121 Time: 0.05655\n",
            "Iteration: 0089 Loss: -0.13136 Time: 0.07661\n",
            "Iteration: 0090 Loss: -0.13146 Time: 0.05295\n",
            "Iteration: 0091 Loss: -0.13154 Time: 0.05625\n",
            "Iteration: 0092 Loss: -0.13161 Time: 0.04764\n",
            "Iteration: 0093 Loss: -0.13168 Time: 0.05565\n",
            "Iteration: 0094 Loss: -0.13175 Time: 0.06200\n",
            "Iteration: 0095 Loss: -0.13184 Time: 0.06251\n",
            "Iteration: 0096 Loss: -0.13194 Time: 0.05391\n",
            "Iteration: 0097 Loss: -0.13204 Time: 0.04799\n",
            "Iteration: 0098 Loss: -0.13215 Time: 0.06154\n",
            "Iteration: 0099 Loss: -0.13225 Time: 0.07201\n",
            "Iteration: 0100 Loss: -0.13233 Time: 0.04954\n",
            "Iteration: 0101 Loss: -0.13240 Time: 0.05201\n",
            "Iteration: 0102 Loss: -0.13246 Time: 0.05751\n",
            "Iteration: 0103 Loss: -0.13252 Time: 0.08036\n",
            "Iteration: 0104 Loss: -0.13259 Time: 0.04900\n",
            "Iteration: 0105 Loss: -0.13266 Time: 0.06949\n",
            "Iteration: 0106 Loss: -0.13273 Time: 0.06653\n",
            "Iteration: 0107 Loss: -0.13281 Time: 0.05368\n",
            "Iteration: 0108 Loss: -0.13289 Time: 0.04951\n",
            "Iteration: 0109 Loss: -0.13296 Time: 0.08159\n",
            "Iteration: 0110 Loss: -0.13303 Time: 0.06701\n",
            "Iteration: 0111 Loss: -0.13308 Time: 0.05152\n",
            "Iteration: 0112 Loss: -0.13314 Time: 0.05033\n",
            "Iteration: 0113 Loss: -0.13320 Time: 0.05853\n",
            "Iteration: 0114 Loss: -0.13326 Time: 0.07194\n",
            "Iteration: 0115 Loss: -0.13332 Time: 0.06195\n",
            "Iteration: 0116 Loss: -0.13338 Time: 0.06855\n",
            "Iteration: 0117 Loss: -0.13344 Time: 0.05100\n",
            "Iteration: 0118 Loss: -0.13350 Time: 0.04653\n",
            "Iteration: 0119 Loss: -0.13355 Time: 0.05213\n",
            "Iteration: 0120 Loss: -0.13361 Time: 0.04947\n",
            "Iteration: 0121 Loss: -0.13366 Time: 0.05806\n",
            "Iteration: 0122 Loss: -0.13371 Time: 0.05752\n",
            "Iteration: 0123 Loss: -0.13376 Time: 0.05156\n",
            "Iteration: 0124 Loss: -0.13382 Time: 0.08007\n",
            "Iteration: 0125 Loss: -0.13387 Time: 0.06847\n",
            "Iteration: 0126 Loss: -0.13392 Time: 0.07617\n",
            "Iteration: 0127 Loss: -0.13397 Time: 0.05200\n",
            "Iteration: 0128 Loss: -0.13402 Time: 0.05354\n",
            "Iteration: 0129 Loss: -0.13407 Time: 0.04973\n",
            "Iteration: 0130 Loss: -0.13412 Time: 0.04861\n",
            "Iteration: 0131 Loss: -0.13417 Time: 0.05200\n",
            "Iteration: 0132 Loss: -0.13422 Time: 0.05541\n",
            "Iteration: 0133 Loss: -0.13426 Time: 0.05912\n",
            "Iteration: 0134 Loss: -0.13431 Time: 0.06652\n",
            "Iteration: 0135 Loss: -0.13436 Time: 0.05052\n",
            "Iteration: 0136 Loss: -0.13441 Time: 0.07300\n",
            "Iteration: 0137 Loss: -0.13445 Time: 0.05353\n",
            "Iteration: 0138 Loss: -0.13450 Time: 0.07057\n",
            "Iteration: 0139 Loss: -0.13454 Time: 0.05021\n",
            "Iteration: 0140 Loss: -0.13458 Time: 0.04868\n",
            "Iteration: 0141 Loss: -0.13463 Time: 0.06501\n",
            "Iteration: 0142 Loss: -0.13467 Time: 0.05552\n",
            "Iteration: 0143 Loss: -0.13471 Time: 0.05100\n",
            "Iteration: 0144 Loss: -0.13476 Time: 0.05058\n",
            "Iteration: 0145 Loss: -0.13480 Time: 0.05294\n",
            "Iteration: 0146 Loss: -0.13484 Time: 0.05713\n",
            "Iteration: 0147 Loss: -0.13488 Time: 0.05000\n",
            "Iteration: 0148 Loss: -0.13492 Time: 0.04952\n",
            "Iteration: 0149 Loss: -0.13496 Time: 0.05800\n",
            "Iteration: 0150 Loss: -0.13500 Time: 0.06551\n",
            "Iteration: 0151 Loss: -0.13504 Time: 0.06138\n",
            "Iteration: 0152 Loss: -0.13508 Time: 0.05500\n",
            "Iteration: 0153 Loss: -0.13512 Time: 0.06055\n",
            "Iteration: 0154 Loss: -0.13516 Time: 0.05295\n",
            "Iteration: 0155 Loss: -0.13519 Time: 0.06253\n",
            "Iteration: 0156 Loss: -0.13523 Time: 0.05411\n",
            "Iteration: 0157 Loss: -0.13527 Time: 0.04811\n",
            "Iteration: 0158 Loss: -0.13530 Time: 0.04860\n",
            "Iteration: 0159 Loss: -0.13534 Time: 0.05605\n",
            "Iteration: 0160 Loss: -0.13537 Time: 0.05033\n",
            "Iteration: 0161 Loss: -0.13541 Time: 0.07501\n",
            "Iteration: 0162 Loss: -0.13545 Time: 0.04952\n",
            "Iteration: 0163 Loss: -0.13548 Time: 0.08268\n",
            "Iteration: 0164 Loss: -0.13552 Time: 0.05199\n",
            "Iteration: 0165 Loss: -0.13555 Time: 0.05922\n",
            "Iteration: 0166 Loss: -0.13558 Time: 0.05457\n",
            "Iteration: 0167 Loss: -0.13562 Time: 0.05144\n",
            "Iteration: 0168 Loss: -0.13565 Time: 0.05571\n",
            "Iteration: 0169 Loss: -0.13568 Time: 0.06012\n",
            "Iteration: 0170 Loss: -0.13572 Time: 0.05557\n",
            "Iteration: 0171 Loss: -0.13575 Time: 0.04700\n",
            "Iteration: 0172 Loss: -0.13578 Time: 0.05357\n",
            "Iteration: 0173 Loss: -0.13582 Time: 0.05578\n",
            "Iteration: 0174 Loss: -0.13585 Time: 0.07454\n",
            "Iteration: 0175 Loss: -0.13588 Time: 0.05101\n",
            "Iteration: 0176 Loss: -0.13591 Time: 0.06709\n",
            "Iteration: 0177 Loss: -0.13594 Time: 0.05845\n",
            "Iteration: 0178 Loss: -0.13599 Time: 0.05700\n",
            "Iteration: 0179 Loss: -0.13601 Time: 0.06854\n",
            "Iteration: 0180 Loss: -0.13604 Time: 0.05304\n",
            "Iteration: 0181 Loss: -0.13607 Time: 0.06876\n",
            "Iteration: 0182 Loss: -0.13610 Time: 0.06756\n",
            "Iteration: 0183 Loss: -0.13613 Time: 0.06306\n",
            "Iteration: 0184 Loss: -0.13616 Time: 0.07017\n",
            "Iteration: 0185 Loss: -0.13619 Time: 0.07358\n",
            "Iteration: 0186 Loss: -0.13622 Time: 0.07253\n",
            "Iteration: 0187 Loss: -0.13625 Time: 0.06505\n",
            "Iteration: 0188 Loss: -0.13628 Time: 0.05248\n",
            "Iteration: 0189 Loss: -0.13631 Time: 0.05000\n",
            "Iteration: 0190 Loss: -0.13634 Time: 0.05753\n",
            "Iteration: 0191 Loss: -0.13637 Time: 0.06100\n",
            "Iteration: 0192 Loss: -0.13640 Time: 0.06556\n",
            "Iteration: 0193 Loss: -0.13643 Time: 0.07486\n",
            "Iteration: 0194 Loss: -0.13646 Time: 0.07454\n",
            "Iteration: 0195 Loss: -0.13649 Time: 0.07383\n",
            "Iteration: 0196 Loss: -0.13652 Time: 0.06552\n",
            "Iteration: 0197 Loss: -0.13655 Time: 0.07116\n",
            "Iteration: 0198 Loss: -0.13658 Time: 0.07497\n",
            "Iteration: 0199 Loss: -0.13660 Time: 0.05246\n",
            "Iteration: 0200 Loss: -0.13663 Time: 0.05290\n",
            "Done! \n",
            "\n",
            "STEP 3/3 - MODEL EVALUATION \n",
            "\n",
            "Computing the final embedding vectors, for evaluation\n",
            "Done! \n",
            "\n",
            "Testing: link prediction\n",
            "Done! \n",
            "\n",
            "Testing: community detection\n",
            "Done! \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "FINAL RESULTS \n",
            "\n",
            "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on blogs\n",
            "All scores reported below are computed over the 1 run(s) \n",
            "\n",
            "Community detection:\n",
            "\n",
            "Mean AMI score: 0.6801481721700621\n",
            "Std of AMI scores: 0.0 \n",
            "\n",
            "Mean ARI score:  0.7812541182205114\n",
            "Std of ARI scores:  0.0 \n",
            "\n",
            "Link prediction:\n",
            "\n",
            "Mean AUC score:  0.9170562283126708\n",
            "Std of AUC scores:  0.0 \n",
            "\n",
            "Mean AP score:  0.9272280020303059\n",
            "Std of AP scores:  0.0 \n",
            " \n",
            "\n",
            "c:\\Users\\Moher\\Personal\\PhD\\Projects\\modularity_aware_gae\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 22:17:28.555990: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\n",
            "2025-02-25 22:17:28.556248: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2025-02-25 22:17:31.987150: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\n",
            "2025-02-25 22:17:32.008473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: NVIDIA GeForce RTX 4080 major: 8 minor: 9 memoryClockRate(GHz): 2.505\n",
            "pciBusID: 0000:01:00.0\n",
            "2025-02-25 22:17:32.009397: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\n",
            "2025-02-25 22:17:32.010313: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found\n",
            "2025-02-25 22:17:32.011107: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found\n",
            "2025-02-25 22:17:32.011781: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found\n",
            "2025-02-25 22:17:32.012491: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found\n",
            "2025-02-25 22:17:32.013346: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found\n",
            "2025-02-25 22:17:32.014072: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\n",
            "2025-02-25 22:17:32.014314: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2025-02-25 22:17:32.015110: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
            "2025-02-25 22:17:32.019166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2025-02-25 22:17:32.019354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
          ]
        }
      ],
      "source": [
        "%cd modularity_aware_gae\n",
        "!python train.py --dataset=blogs --features=False --task=task_2 --model=gcn_vae --iterations=200 --learning_rate=0.01 --hidden=32 --dimension=16 --beta=0.75 --lamb=0.5 --gamma=2 --s_reg=10 --fastgae=False --nb_run=1\n",
        "%cd .."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
