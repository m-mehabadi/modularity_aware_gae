{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Blogs Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Moher\\Personal\\PhD\\Projects\\modularity_aware_gae\\modularity_aware_gae\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "EXPERIMENTAL SETTING \n",
            "\n",
            "- Graph dataset: blogs\n",
            "- Mode name: gcn_vae\n",
            "- Number of models to train: 1\n",
            "- Number of training iterations for each model: 200\n",
            "- Learning rate: 0.01\n",
            "- Dropout rate: 0.0\n",
            "- Use of node features in the input layer: False\n",
            "- Dimension of the GCN hidden layer: 32\n",
            "- Dimension of the output layer: 16\n",
            "- lambda: 0.5\n",
            "- beta: 0.75\n",
            "- gamma: 2.0\n",
            "- s: 10\n",
            "- FastGAE: no \n",
            "\n",
            "Final embedding vectors will be evaluated on:\n",
            "- Task 2, i.e., joint community detection and link prediction\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "LOADING DATA\n",
            "\n",
            "Loading the blogs graph\n",
            "- Number of nodes: 1224\n",
            "- Number of communities: 2\n",
            "- Use of node features: False\n",
            "Done! \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "EXPERIMENTS ON MODEL 1 / 1 \n",
            "\n",
            "STEP 1/3 - PREPROCESSING STEPS \n",
            "\n",
            "Masking some edges from the training graph, for link prediction\n",
            "(validation set: 5.0 % of edges - test set: 10.0 % of edges)\n",
            "Done! \n",
            "\n",
            "Preprocessing node features\n",
            "Done! \n",
            "\n",
            "Running the Louvain algorithm for community detection\n",
            "as a preprocessing step for the encoder\n",
            "Done! Louvain has found 40 communities \n",
            "\n",
            "Setting up the model and the optimizer\n",
            "Done! \n",
            "\n",
            "Preprocessing on message passing matrices\n",
            "Done! \n",
            "\n",
            "Initializing TF session\n",
            "Done! \n",
            "\n",
            "STEP 2/3 - MODEL TRAINING \n",
            "\n",
            "Starting training\n",
            "Iteration: 0001 Loss: 0.09372 Time: 0.25788\n",
            "Iteration: 0002 Loss: 0.09340 Time: 0.04900\n",
            "Iteration: 0003 Loss: 0.09192 Time: 0.05996\n",
            "Iteration: 0004 Loss: 0.08837 Time: 0.05105\n",
            "Iteration: 0005 Loss: 0.08174 Time: 0.06847\n",
            "Iteration: 0006 Loss: 0.07105 Time: 0.06758\n",
            "Iteration: 0007 Loss: 0.05576 Time: 0.06500\n",
            "Iteration: 0008 Loss: 0.03653 Time: 0.06048\n",
            "Iteration: 0009 Loss: 0.01647 Time: 0.05856\n",
            "Iteration: 0010 Loss: 0.00259 Time: 0.06660\n",
            "Iteration: 0011 Loss: 0.00414 Time: 0.05659\n",
            "Iteration: 0012 Loss: 0.01512 Time: 0.06101\n",
            "Iteration: 0013 Loss: 0.01646 Time: 0.05427\n",
            "Iteration: 0014 Loss: 0.00808 Time: 0.07218\n",
            "Iteration: 0015 Loss: -0.00249 Time: 0.06958\n",
            "Iteration: 0016 Loss: -0.00976 Time: 0.05752\n",
            "Iteration: 0017 Loss: -0.01224 Time: 0.05303\n",
            "Iteration: 0018 Loss: -0.01156 Time: 0.05123\n",
            "Iteration: 0019 Loss: -0.01055 Time: 0.04605\n",
            "Iteration: 0020 Loss: -0.01105 Time: 0.04847\n",
            "Iteration: 0021 Loss: -0.01360 Time: 0.05653\n",
            "Iteration: 0022 Loss: -0.01786 Time: 0.07800\n",
            "Iteration: 0023 Loss: -0.02297 Time: 0.07560\n",
            "Iteration: 0024 Loss: -0.02785 Time: 0.05259\n",
            "Iteration: 0025 Loss: -0.03160 Time: 0.05800\n",
            "Iteration: 0026 Loss: -0.03393 Time: 0.06346\n",
            "Iteration: 0027 Loss: -0.03538 Time: 0.04999\n",
            "Iteration: 0028 Loss: -0.03717 Time: 0.07156\n",
            "Iteration: 0029 Loss: -0.04015 Time: 0.06654\n",
            "Iteration: 0030 Loss: -0.04397 Time: 0.06901\n",
            "Iteration: 0031 Loss: -0.04733 Time: 0.05052\n",
            "Iteration: 0032 Loss: -0.04912 Time: 0.05100\n",
            "Iteration: 0033 Loss: -0.05027 Time: 0.05102\n",
            "Iteration: 0034 Loss: -0.05247 Time: 0.05410\n",
            "Iteration: 0035 Loss: -0.05577 Time: 0.05021\n",
            "Iteration: 0036 Loss: -0.05920 Time: 0.05301\n",
            "Iteration: 0037 Loss: -0.06213 Time: 0.05152\n",
            "Iteration: 0038 Loss: -0.06502 Time: 0.04901\n",
            "Iteration: 0039 Loss: -0.06878 Time: 0.04820\n",
            "Iteration: 0040 Loss: -0.07376 Time: 0.05501\n",
            "Iteration: 0041 Loss: -0.07965 Time: 0.05844\n",
            "Iteration: 0042 Loss: -0.08609 Time: 0.05253\n",
            "Iteration: 0043 Loss: -0.09303 Time: 0.05700\n",
            "Iteration: 0044 Loss: -0.10059 Time: 0.04838\n",
            "Iteration: 0045 Loss: -0.10829 Time: 0.05017\n",
            "Iteration: 0046 Loss: -0.11500 Time: 0.04653\n",
            "Iteration: 0047 Loss: -0.11931 Time: 0.05300\n",
            "Iteration: 0048 Loss: -0.12030 Time: 0.05052\n",
            "Iteration: 0049 Loss: -0.11822 Time: 0.05000\n",
            "Iteration: 0050 Loss: -0.11499 Time: 0.06356\n",
            "Iteration: 0051 Loss: -0.11302 Time: 0.04865\n",
            "Iteration: 0052 Loss: -0.11352 Time: 0.05052\n",
            "Iteration: 0053 Loss: -0.11611 Time: 0.07453\n",
            "Iteration: 0054 Loss: -0.11954 Time: 0.05000\n",
            "Iteration: 0055 Loss: -0.12260 Time: 0.05351\n",
            "Iteration: 0056 Loss: -0.12464 Time: 0.06101\n",
            "Iteration: 0057 Loss: -0.12561 Time: 0.05549\n",
            "Iteration: 0058 Loss: -0.12585 Time: 0.06500\n",
            "Iteration: 0059 Loss: -0.12572 Time: 0.05852\n",
            "Iteration: 0060 Loss: -0.12539 Time: 0.05129\n",
            "Iteration: 0061 Loss: -0.12497 Time: 0.04917\n",
            "Iteration: 0062 Loss: -0.12467 Time: 0.07154\n",
            "Iteration: 0063 Loss: -0.12472 Time: 0.05600\n",
            "Iteration: 0064 Loss: -0.12522 Time: 0.05691\n",
            "Iteration: 0065 Loss: -0.12609 Time: 0.06307\n",
            "Iteration: 0066 Loss: -0.12713 Time: 0.06352\n",
            "Iteration: 0067 Loss: -0.12812 Time: 0.06173\n",
            "Iteration: 0068 Loss: -0.12887 Time: 0.06300\n",
            "Iteration: 0069 Loss: -0.12933 Time: 0.05662\n",
            "Iteration: 0070 Loss: -0.12949 Time: 0.05200\n",
            "Iteration: 0071 Loss: -0.12944 Time: 0.05051\n",
            "Iteration: 0072 Loss: -0.12929 Time: 0.04800\n",
            "Iteration: 0073 Loss: -0.12916 Time: 0.05028\n",
            "Iteration: 0074 Loss: -0.12915 Time: 0.04758\n",
            "Iteration: 0075 Loss: -0.12929 Time: 0.04746\n",
            "Iteration: 0076 Loss: -0.12957 Time: 0.05000\n",
            "Iteration: 0077 Loss: -0.12994 Time: 0.05535\n",
            "Iteration: 0078 Loss: -0.13030 Time: 0.04800\n",
            "Iteration: 0079 Loss: -0.13060 Time: 0.04751\n",
            "Iteration: 0080 Loss: -0.13079 Time: 0.05600\n",
            "Iteration: 0081 Loss: -0.13086 Time: 0.05728\n",
            "Iteration: 0082 Loss: -0.13086 Time: 0.05227\n",
            "Iteration: 0083 Loss: -0.13086 Time: 0.05100\n",
            "Iteration: 0084 Loss: -0.13090 Time: 0.06254\n",
            "Iteration: 0085 Loss: -0.13101 Time: 0.05400\n",
            "Iteration: 0086 Loss: -0.13117 Time: 0.05153\n",
            "Iteration: 0087 Loss: -0.13135 Time: 0.05900\n",
            "Iteration: 0088 Loss: -0.13153 Time: 0.04752\n",
            "Iteration: 0089 Loss: -0.13169 Time: 0.07554\n",
            "Iteration: 0090 Loss: -0.13182 Time: 0.05100\n",
            "Iteration: 0091 Loss: -0.13191 Time: 0.05244\n",
            "Iteration: 0092 Loss: -0.13198 Time: 0.05601\n",
            "Iteration: 0093 Loss: -0.13203 Time: 0.07957\n",
            "Iteration: 0094 Loss: -0.13208 Time: 0.05500\n",
            "Iteration: 0095 Loss: -0.13215 Time: 0.06208\n",
            "Iteration: 0096 Loss: -0.13224 Time: 0.05720\n",
            "Iteration: 0097 Loss: -0.13235 Time: 0.04800\n",
            "Iteration: 0098 Loss: -0.13246 Time: 0.05458\n",
            "Iteration: 0099 Loss: -0.13256 Time: 0.05547\n",
            "Iteration: 0100 Loss: -0.13264 Time: 0.06954\n",
            "Iteration: 0101 Loss: -0.13270 Time: 0.05101\n",
            "Iteration: 0102 Loss: -0.13276 Time: 0.05057\n",
            "Iteration: 0103 Loss: -0.13281 Time: 0.04951\n",
            "Iteration: 0104 Loss: -0.13287 Time: 0.05853\n",
            "Iteration: 0105 Loss: -0.13294 Time: 0.04852\n",
            "Iteration: 0106 Loss: -0.13301 Time: 0.04600\n",
            "Iteration: 0107 Loss: -0.13309 Time: 0.04800\n",
            "Iteration: 0108 Loss: -0.13317 Time: 0.05052\n",
            "Iteration: 0109 Loss: -0.13323 Time: 0.05301\n",
            "Iteration: 0110 Loss: -0.13329 Time: 0.05051\n",
            "Iteration: 0111 Loss: -0.13335 Time: 0.04601\n",
            "Iteration: 0112 Loss: -0.13340 Time: 0.05551\n",
            "Iteration: 0113 Loss: -0.13346 Time: 0.04738\n",
            "Iteration: 0114 Loss: -0.13352 Time: 0.05092\n",
            "Iteration: 0115 Loss: -0.13358 Time: 0.05010\n",
            "Iteration: 0116 Loss: -0.13364 Time: 0.05001\n",
            "Iteration: 0117 Loss: -0.13370 Time: 0.05257\n",
            "Iteration: 0118 Loss: -0.13375 Time: 0.05000\n",
            "Iteration: 0119 Loss: -0.13380 Time: 0.05148\n",
            "Iteration: 0120 Loss: -0.13385 Time: 0.07300\n",
            "Iteration: 0121 Loss: -0.13390 Time: 0.04952\n",
            "Iteration: 0122 Loss: -0.13395 Time: 0.05000\n",
            "Iteration: 0123 Loss: -0.13400 Time: 0.04667\n",
            "Iteration: 0124 Loss: -0.13405 Time: 0.04900\n",
            "Iteration: 0125 Loss: -0.13411 Time: 0.04887\n",
            "Iteration: 0126 Loss: -0.13416 Time: 0.04914\n",
            "Iteration: 0127 Loss: -0.13420 Time: 0.04824\n",
            "Iteration: 0128 Loss: -0.13425 Time: 0.06195\n",
            "Iteration: 0129 Loss: -0.13430 Time: 0.05407\n",
            "Iteration: 0130 Loss: -0.13434 Time: 0.05300\n",
            "Iteration: 0131 Loss: -0.13439 Time: 0.04619\n",
            "Iteration: 0132 Loss: -0.13443 Time: 0.06238\n",
            "Iteration: 0133 Loss: -0.13448 Time: 0.06554\n",
            "Iteration: 0134 Loss: -0.13452 Time: 0.05564\n",
            "Iteration: 0135 Loss: -0.13457 Time: 0.05539\n",
            "Iteration: 0136 Loss: -0.13461 Time: 0.05213\n",
            "Iteration: 0137 Loss: -0.13465 Time: 0.05105\n",
            "Iteration: 0138 Loss: -0.13470 Time: 0.04848\n",
            "Iteration: 0139 Loss: -0.13474 Time: 0.06005\n",
            "Iteration: 0140 Loss: -0.13478 Time: 0.06100\n",
            "Iteration: 0141 Loss: -0.13482 Time: 0.05300\n",
            "Iteration: 0142 Loss: -0.13486 Time: 0.06799\n",
            "Iteration: 0143 Loss: -0.13491 Time: 0.06755\n",
            "Iteration: 0144 Loss: -0.13495 Time: 0.05500\n",
            "Iteration: 0145 Loss: -0.13499 Time: 0.05952\n",
            "Iteration: 0146 Loss: -0.13502 Time: 0.05164\n",
            "Iteration: 0147 Loss: -0.13506 Time: 0.05002\n",
            "Iteration: 0148 Loss: -0.13510 Time: 0.05056\n",
            "Iteration: 0149 Loss: -0.13514 Time: 0.06777\n",
            "Iteration: 0150 Loss: -0.13518 Time: 0.06383\n",
            "Iteration: 0151 Loss: -0.13522 Time: 0.06696\n",
            "Iteration: 0152 Loss: -0.13525 Time: 0.05058\n",
            "Iteration: 0153 Loss: -0.13529 Time: 0.05223\n",
            "Iteration: 0154 Loss: -0.13533 Time: 0.06370\n",
            "Iteration: 0155 Loss: -0.13537 Time: 0.05057\n",
            "Iteration: 0156 Loss: -0.13540 Time: 0.04983\n",
            "Iteration: 0157 Loss: -0.13544 Time: 0.05352\n",
            "Iteration: 0158 Loss: -0.13548 Time: 0.05100\n",
            "Iteration: 0159 Loss: -0.13551 Time: 0.07130\n",
            "Iteration: 0160 Loss: -0.13554 Time: 0.05100\n",
            "Iteration: 0161 Loss: -0.13558 Time: 0.06246\n",
            "Iteration: 0162 Loss: -0.13561 Time: 0.05388\n",
            "Iteration: 0163 Loss: -0.13565 Time: 0.07700\n",
            "Iteration: 0164 Loss: -0.13568 Time: 0.05751\n",
            "Iteration: 0165 Loss: -0.13572 Time: 0.07162\n",
            "Iteration: 0166 Loss: -0.13575 Time: 0.07177\n",
            "Iteration: 0167 Loss: -0.13578 Time: 0.04951\n",
            "Iteration: 0168 Loss: -0.13582 Time: 0.04699\n",
            "Iteration: 0169 Loss: -0.13585 Time: 0.05651\n",
            "Iteration: 0170 Loss: -0.13588 Time: 0.05200\n",
            "Iteration: 0171 Loss: -0.13592 Time: 0.04751\n",
            "Iteration: 0172 Loss: -0.13595 Time: 0.04871\n",
            "Iteration: 0173 Loss: -0.13598 Time: 0.05008\n",
            "Iteration: 0174 Loss: -0.13601 Time: 0.05188\n",
            "Iteration: 0175 Loss: -0.13605 Time: 0.05957\n",
            "Iteration: 0176 Loss: -0.13608 Time: 0.05104\n",
            "Iteration: 0177 Loss: -0.13611 Time: 0.06105\n",
            "Iteration: 0178 Loss: -0.13614 Time: 0.04952\n",
            "Iteration: 0179 Loss: -0.13617 Time: 0.04800\n",
            "Iteration: 0180 Loss: -0.13620 Time: 0.06850\n",
            "Iteration: 0181 Loss: -0.13624 Time: 0.05100\n",
            "Iteration: 0182 Loss: -0.13627 Time: 0.05152\n",
            "Iteration: 0183 Loss: -0.13630 Time: 0.05000\n",
            "Iteration: 0184 Loss: -0.13633 Time: 0.05252\n",
            "Iteration: 0185 Loss: -0.13636 Time: 0.06602\n",
            "Iteration: 0186 Loss: -0.13639 Time: 0.04809\n",
            "Iteration: 0187 Loss: -0.13642 Time: 0.04820\n",
            "Iteration: 0188 Loss: -0.13645 Time: 0.06557\n",
            "Iteration: 0189 Loss: -0.13648 Time: 0.06256\n",
            "Iteration: 0190 Loss: -0.13652 Time: 0.04867\n",
            "Iteration: 0191 Loss: -0.13655 Time: 0.04848\n",
            "Iteration: 0192 Loss: -0.13658 Time: 0.04975\n",
            "Iteration: 0193 Loss: -0.13661 Time: 0.05231\n",
            "Iteration: 0194 Loss: -0.13664 Time: 0.04727\n",
            "Iteration: 0195 Loss: -0.13667 Time: 0.06889\n",
            "Iteration: 0196 Loss: -0.13670 Time: 0.05196\n",
            "Iteration: 0197 Loss: -0.13673 Time: 0.05756\n",
            "Iteration: 0198 Loss: -0.13676 Time: 0.05953\n",
            "Iteration: 0199 Loss: -0.13679 Time: 0.05849\n",
            "Iteration: 0200 Loss: -0.13682 Time: 0.05114\n",
            "Done! \n",
            "\n",
            "STEP 3/3 - MODEL EVALUATION \n",
            "\n",
            "Computing the final embedding vectors, for evaluation\n",
            "Done! \n",
            "\n",
            "Testing: link prediction\n",
            "Done! \n",
            "\n",
            "Testing: community detection\n",
            "Done! \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "FINAL RESULTS \n",
            "\n",
            "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on blogs\n",
            "All scores reported below are computed over the 1 run(s) \n",
            "\n",
            "Community detection:\n",
            "\n",
            "Mean AMI score: 0.7041548239385753\n",
            "Std of AMI scores: 0.0 \n",
            "\n",
            "Mean ARI score:  0.8016235848740431\n",
            "Std of ARI scores:  0.0 \n",
            "\n",
            "Link prediction:\n",
            "\n",
            "Mean AUC score:  0.9151570369463093\n",
            "Std of AUC scores:  0.0 \n",
            "\n",
            "Mean AP score:  0.9235037374834958\n",
            "Std of AP scores:  0.0 \n",
            " \n",
            "\n",
            "c:\\Users\\Moher\\Personal\\PhD\\Projects\\modularity_aware_gae\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 22:22:23.858694: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\n",
            "2025-02-25 22:22:23.858950: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2025-02-25 22:22:27.305139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\n",
            "2025-02-25 22:22:27.327317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: NVIDIA GeForce RTX 4080 major: 8 minor: 9 memoryClockRate(GHz): 2.505\n",
            "pciBusID: 0000:01:00.0\n",
            "2025-02-25 22:22:27.328268: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\n",
            "2025-02-25 22:22:27.329062: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found\n",
            "2025-02-25 22:22:27.329772: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found\n",
            "2025-02-25 22:22:27.330659: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found\n",
            "2025-02-25 22:22:27.331425: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found\n",
            "2025-02-25 22:22:27.332148: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found\n",
            "2025-02-25 22:22:27.332864: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\n",
            "2025-02-25 22:22:27.333066: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2025-02-25 22:22:27.333789: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
            "2025-02-25 22:22:27.336898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2025-02-25 22:22:27.337094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
          ]
        }
      ],
      "source": [
        "%cd modularity_aware_gae\n",
        "!python train.py --dataset=blogs --features=False --task=task_2 --model=gcn_vae --iterations=200 --learning_rate=0.01 --hidden=32 --dimension=16 --beta=0.75 --lamb=0.5 --gamma=2 --s_reg=10 --fastgae=False --nb_run=1\n",
        "%cd .."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
